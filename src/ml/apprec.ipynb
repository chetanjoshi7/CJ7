{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "epochs = 10\n",
    "display_step = 10\n",
    "\n",
    "learning_rate = 0.3\n",
    "\n",
    "batch_size = 250\n",
    "\n",
    "sample_data = \"./ratings.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USERS: 6040 ITEMS: 3706\n"
     ]
    }
   ],
   "source": [
    "# Reading dataset\n",
    "\n",
    "df = pd.read_csv(sample_data, sep='t', names=['user', 'item', 'rating', 'timestamp'], header=None)\n",
    "\n",
    "y = df.timestamp\n",
    "df = df.drop('timestamp', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)\n",
    "\n",
    "train_data = X_train\n",
    "test_data = X_test\n",
    "\n",
    "num_items = df.item.nunique()\n",
    "num_users = df.user.nunique()\n",
    "\n",
    "print(\"USERS: {} ITEMS: {}\".format(num_users, num_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize in [0, 1]\n",
    "\n",
    "r = df['rating'].values.astype(float)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(r.reshape(-1,1))\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "df['rating'] = df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame in user-item matrix\n",
    "\n",
    "matrix = df.pivot(index='user', columns='item', values='rating')\n",
    "matrix.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (6040, 3706)\n"
     ]
    }
   ],
   "source": [
    "# Users and items ordered as they are in matrix\n",
    "\n",
    "users = matrix.index.tolist()\n",
    "items = matrix.columns.tolist()\n",
    "\n",
    "matrix = matrix.values\n",
    "\n",
    "print(\"Matrix shape: {}\".format(matrix.shape))\n",
    "\n",
    "# num_users = matrix.shape[0]\n",
    "# num_items = matrix.shape[1]\n",
    "# print(\"USERS: {} ITEMS: {}\".format(num_users, num_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "\n",
    "num_input = num_items   # num of items\n",
    "num_hidden_1 = 10       # 1st layer num features\n",
    "num_hidden_2 = 5        # 2nd layer num features (the latent dim)\n",
    "\n",
    "X = tf.placeholder(tf.float64, [None, num_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Building the decoder\n",
    "\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "\n",
    "y_pred = decoder_op\n",
    "\n",
    "# Targets are the input data.\n",
    "\n",
    "y_true = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer, minimize the squared error\n",
    "\n",
    "loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "# Define evaluation metrics\n",
    "\n",
    "eval_x = tf.placeholder(tf.int32, )\n",
    "eval_y = tf.placeholder(tf.int32, )\n",
    "pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.34608616928259534\n",
      "Epoch: 2 Loss: 0.3415129780769348\n",
      "Epoch: 3 Loss: 0.3238331501682599\n",
      "Epoch: 4 Loss: 0.28673282638192177\n",
      "Epoch: 5 Loss: 0.26592473685741425\n",
      "Epoch: 6 Loss: 0.21411477526028952\n",
      "Epoch: 7 Loss: 0.09699901767695944\n",
      "Epoch: 8 Loss: 0.024531338984767597\n",
      "Epoch: 9 Loss: 0.0202350909045587\n",
      "Epoch: 10 Loss: 0.019465544959530234\n",
      "Predictions...\n",
      "          user  item    rating\n",
      "0            1     1  0.023705\n",
      "1            1     2  0.048517\n",
      "2            1     3  0.023870\n",
      "3            1     4  0.012606\n",
      "4            1     5  0.020300\n",
      "5            1     6  0.109864\n",
      "6            1     7  0.044290\n",
      "7            1     8  0.009958\n",
      "8            1     9  0.013474\n",
      "9            1    10  0.101964\n",
      "10           1    11  0.107748\n",
      "11           1    12  0.006487\n",
      "12           1    13  0.013581\n",
      "13           1    14  0.017163\n",
      "14           1    15  0.000450\n",
      "15           1    16  0.072607\n",
      "16           1    17  0.172042\n",
      "17           1    18  0.013385\n",
      "18           1    19  0.013850\n",
      "19           1    20  0.008471\n",
      "20           1    21  0.146342\n",
      "21           1    22  0.035855\n",
      "22           1    23  0.009331\n",
      "23           1    24  0.044618\n",
      "24           1    25  0.094950\n",
      "25           1    26  0.010963\n",
      "26           1    27  0.013635\n",
      "27           1    28  0.023331\n",
      "28           1    29  0.049962\n",
      "29           1    30  0.001477\n",
      "...        ...   ...       ...\n",
      "22384210  6040  3923  0.016178\n",
      "22384211  6040  3924  0.015086\n",
      "22384212  6040  3925  0.021540\n",
      "22384213  6040  3926  0.013311\n",
      "22384214  6040  3927  0.028993\n",
      "22384215  6040  3928  0.019782\n",
      "22384216  6040  3929  0.023624\n",
      "22384217  6040  3930  0.018288\n",
      "22384218  6040  3931  0.016700\n",
      "22384219  6040  3932  0.026788\n",
      "22384220  6040  3933  0.015965\n",
      "22384221  6040  3934  0.014582\n",
      "22384222  6040  3935  0.013577\n",
      "22384223  6040  3936  0.019843\n",
      "22384224  6040  3937  0.021112\n",
      "22384225  6040  3938  0.012848\n",
      "22384226  6040  3939  0.015711\n",
      "22384227  6040  3940  0.008930\n",
      "22384228  6040  3941  0.015755\n",
      "22384229  6040  3942  0.015436\n",
      "22384230  6040  3943  0.018622\n",
      "22384231  6040  3944  0.008434\n",
      "22384232  6040  3945  0.013499\n",
      "22384233  6040  3946  0.015917\n",
      "22384234  6040  3947  0.016618\n",
      "22384235  6040  3948  0.053548\n",
      "22384236  6040  3949  0.029021\n",
      "22384237  6040  3950  0.018839\n",
      "22384238  6040  3951  0.016528\n",
      "22384239  6040  3952  0.029790\n",
      "\n",
      "[22384240 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    session.run(local_init)\n",
    "\n",
    "    num_batches = int(matrix.shape[0] / batch_size)\n",
    "    matrix = np.array_split(matrix, num_batches)\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        avg_cost = 0\n",
    "\n",
    "        for batch in matrix:\n",
    "            _, l = session.run([optimizer, loss], feed_dict={X: batch})\n",
    "            avg_cost += l\n",
    "\n",
    "        avg_cost /= num_batches\n",
    "\n",
    "        print(\"Epoch: {} Loss: {}\".format(i + 1, avg_cost))\n",
    "\n",
    "        # if i % display_step == 0 or i == 1:\n",
    "        #     print('Step %i: Minibatch Loss: %f' % (i, l))\n",
    "\n",
    "    print(\"Predictions...\")\n",
    "\n",
    "    matrix = np.concatenate(matrix, axis=0)\n",
    "\n",
    "    preds = session.run(decoder_op, feed_dict={X: matrix})\n",
    "\n",
    "    # print(matrix)\n",
    "    # print(preds)\n",
    "    \n",
    "    predictions = predictions.append(pd.DataFrame(preds))\n",
    "\n",
    "    predictions = predictions.stack().reset_index(name='rating')\n",
    "    predictions.columns = ['user', 'item', 'rating']\n",
    "    predictions['user'] = predictions['user'].map(lambda value: users[value])\n",
    "    predictions['item'] = predictions['item'].map(lambda value: items[value])\n",
    "\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out items in training set\n"
     ]
    }
   ],
   "source": [
    "    print(\"Filtering out items in training set\")\n",
    "\n",
    "    keys = ['user', 'item']\n",
    "    i1 = predictions.set_index(keys).index\n",
    "    i2 = df.set_index(keys).index\n",
    "\n",
    "    recs = predictions[~i1.isin(i2)]\n",
    "    recs = recs.sort_values(['user', 'rating'], ascending=[True, False])\n",
    "    recs = recs.groupby('user').head(k)\n",
    "    recs.to_csv('recs.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    }
   ],
   "source": [
    "    # creare un vettore dove ci sono per ogni utente i suoi 10 movies\n",
    "\n",
    "    test = test_data\n",
    "\n",
    "    test = test.sort_values(['user', 'rating'], ascending=[True, False])\n",
    "\n",
    "    #test = test.groupby('user').head(k) #.reset_index(drop=True)\n",
    "    #test_list = test.as_matrix(columns=['item']).reshape((-1))\n",
    "    #recs_list = recs.groupby('user').head(k).as_matrix(columns=['item']).reshape((-1))\n",
    "\n",
    "    print(\"Evaluating...\")\n",
    "\n",
    "    p = 0.0\n",
    "    for user in users[:10]:\n",
    "        test_list = test[(test.user == user)].head(k).values.flatten()\n",
    "        recs_list = recs[(recs.user == user)].head(k).values.flatten()\n",
    "\n",
    "    #session.run(pre_op, feed_dict={eval_x: test_list, eval_y: recs_list})\n",
    "\n",
    "        #pu = precision_score(test_list, recs_list, average='micro')\n",
    "        #p += pu\n",
    "\n",
    "        # print(\"Precision for user {}: {}\".format(user, pu))\n",
    "        # print(\"User test: {}\".format(test_list))\n",
    "        # print(\"User recs: {}\".format(recs_list))\n",
    "\n",
    "    #p /= len(users)\n",
    "\n",
    "    # p = session.run(pre)\n",
    "    # print(\"Precision@{}: {}\".format(k, p))\n",
    "\n",
    "    # print(\"test len: {} - recs len: {}\".format(len(test_list), len(recs_list)))\n",
    "    #\n",
    "    # print(\"test list - type: {}\".format(type(test_list)))\n",
    "    # print(test_list)\n",
    "    #\n",
    "    # print(\"recs list - type: {}\".format(type(recs_list)))\n",
    "    # print(recs_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
