{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Recommender System in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 900\n",
    "debug = False\n",
    "\n",
    "epochs = 500\n",
    "display_step = 10\n",
    "\n",
    "learning_rate = 0.2\n",
    "\n",
    "batch_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_reviews = 'SELECT user_id, book_id, rating, date_created FROM public.\"Reviews\"'\n",
    "\n",
    "sql_books = 'SELECT book_id FROM public.\"Books\"'\n",
    "\n",
    "engine = create_engine('postgresql://ece651_ml:TVL3MV0mguz0DOhLbbm2@localhost:5432/ece651')\n",
    "\n",
    "df = pd.pandas.read_sql(sql_reviews, engine)\n",
    "df_books = pd.pandas.read_sql(sql_books, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = df_books.set_index('book_id').index\n",
    "i2 = df.set_index('book_id').index\n",
    "books = df_books[~i1.isin(i2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1052, 4)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, column = books.shape\n",
    "empty_array = np.zeros((rows, 1))\n",
    "unrated_books = np.hstack((empty_array, books.values, empty_array, empty_array))\n",
    "unrated_books = pd.DataFrame(unrated_books)\n",
    "unrated_books.columns = ['user_id', 'book_id', 'rating', 'date_created']\n",
    "\n",
    "df = df.append(unrated_books, ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Dataset and splitting it in a training set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.date_created\n",
    "df = df.drop('date_created', axis=1)\n",
    "\n",
    "df.columns = ['user', 'book', 'rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)\n",
    "\n",
    "train_data = X_train\n",
    "test_data = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USERS: 149 BOOKS: 866\n"
     ]
    }
   ],
   "source": [
    "num_books = df.book.nunique()\n",
    "num_users = df.user.nunique()\n",
    "\n",
    "print(\"USERS: {} BOOKS: {}\".format(num_users, num_books))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training set with three columns: user, book and ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize in [0, 1]\n",
    "\n",
    "u = df['user'].values.astype(float)\n",
    "\n",
    "user_min = u.min()\n",
    "user_range = u.max() - u.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(u.reshape(-1,1))\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "df['user'] = df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = df['book'].values.astype(float)\n",
    "\n",
    "book_min = b.min()\n",
    "book_range = b.max() - b.min()\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(b.reshape(-1,1))\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "df['book'] = df_normalized\n",
    "\n",
    "if debug:\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = df['rating'].values.astype(float)\n",
    "\n",
    "rating_min = r.min()\n",
    "rating_range = r.max() - r.min()\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(r.reshape(-1,1))\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "df['rating'] = df_normalized\n",
    "\n",
    "if debug:\n",
    "    df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert DataFrame in user-item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = df.pivot(index='user', columns='book', values='rating')\n",
    "matrix.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149, 866)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    for user in range(0, 148):\n",
    "        for book in range(0, 865):\n",
    "            if matrix.iloc[user, book] != 0.0:\n",
    "                print(user,book, matrix.iloc[user,book])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users and items ordered as they are in matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (149, 866)\n"
     ]
    }
   ],
   "source": [
    "users = matrix.index.tolist()\n",
    "books = matrix.columns.tolist()\n",
    "\n",
    "matrix = matrix.values\n",
    "\n",
    "print(\"Matrix shape: {}\".format(matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = num_books   # num of items\n",
    "num_hidden_1 = 10       # 1st layer num features\n",
    "num_hidden_2 = 5        # 2nd layer num features (the latent dim)\n",
    "\n",
    "X = tf.placeholder(tf.float64, [None, num_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = decoder_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targets are the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer, minimize the squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "predictions = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_x = tf.placeholder(tf.int32, )\n",
    "eval_y = tf.placeholder(tf.int32, )\n",
    "pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.3485371702247196\n",
      "Epoch: 2 Loss: 0.3477831648455726\n",
      "Epoch: 3 Loss: 0.3465774688455794\n",
      "Epoch: 4 Loss: 0.34465524223115707\n",
      "Epoch: 5 Loss: 0.34160438511106705\n",
      "Epoch: 6 Loss: 0.3367960618601905\n",
      "Epoch: 7 Loss: 0.32930441697438556\n",
      "Epoch: 8 Loss: 0.3179060187604692\n",
      "Epoch: 9 Loss: 0.30171798004044426\n",
      "Epoch: 10 Loss: 0.2821054723527696\n",
      "Epoch: 11 Loss: 0.2608022838830948\n",
      "Epoch: 12 Loss: 0.2330661184257931\n",
      "Epoch: 13 Loss: 0.18925084173679352\n",
      "Epoch: 14 Loss: 0.1267265205581983\n",
      "Epoch: 15 Loss: 0.0661082412633631\n",
      "Epoch: 16 Loss: 0.03705949294898245\n",
      "Epoch: 17 Loss: 0.02280322007007069\n",
      "Epoch: 18 Loss: 0.01500691877057155\n",
      "Epoch: 19 Loss: 0.011917385790083144\n",
      "Epoch: 20 Loss: 0.009301632901446687\n",
      "Epoch: 21 Loss: 0.0057995235288722646\n",
      "Epoch: 22 Loss: 0.003933653908057345\n",
      "Epoch: 23 Loss: 0.002889335621148348\n",
      "Epoch: 24 Loss: 0.0026650927029550076\n",
      "Epoch: 25 Loss: 0.0026395761200951207\n",
      "Epoch: 26 Loss: 0.0026274845262782443\n",
      "Epoch: 27 Loss: 0.002617068640473816\n",
      "Epoch: 28 Loss: 0.0020886011318200165\n",
      "Epoch: 29 Loss: 0.001451436469020943\n",
      "Epoch: 30 Loss: 0.0014488205326617593\n",
      "Epoch: 31 Loss: 0.0014537458320976132\n",
      "Epoch: 32 Loss: 0.001447334833857086\n",
      "Epoch: 33 Loss: 0.0014463338690499465\n",
      "Epoch: 34 Loss: 0.0014424528473884696\n",
      "Epoch: 35 Loss: 0.00143993311535774\n",
      "Epoch: 36 Loss: 0.0014464287022646102\n",
      "Epoch: 37 Loss: 0.0014403141621086332\n",
      "Epoch: 38 Loss: 0.0014394029470471044\n",
      "Epoch: 39 Loss: 0.001448581726031585\n",
      "Epoch: 40 Loss: 0.001438917449882461\n",
      "Epoch: 41 Loss: 0.0014386982721690503\n",
      "Epoch: 42 Loss: 0.0014392635591017704\n",
      "Epoch: 43 Loss: 0.0014427022930855553\n",
      "Epoch: 44 Loss: 0.0014372808574181464\n",
      "Epoch: 45 Loss: 0.001436856646452927\n",
      "Epoch: 46 Loss: 0.001437427191477683\n",
      "Epoch: 47 Loss: 0.0014374410125633909\n",
      "Epoch: 48 Loss: 0.0014357086620293558\n",
      "Epoch: 49 Loss: 0.0014466663447415663\n",
      "Epoch: 50 Loss: 0.0014361406340160305\n",
      "Epoch: 51 Loss: 0.0014377414805090262\n",
      "Epoch: 52 Loss: 0.0014373459659206371\n",
      "Epoch: 53 Loss: 0.0014341466077085999\n",
      "Epoch: 54 Loss: 0.0014381087320442828\n",
      "Epoch: 55 Loss: 0.0014373943558894098\n",
      "Epoch: 56 Loss: 0.0014363937079906464\n",
      "Epoch: 57 Loss: 0.00143609458528873\n",
      "Epoch: 58 Loss: 0.0014401679638669724\n",
      "Epoch: 59 Loss: 0.0014358395193186072\n",
      "Epoch: 60 Loss: 0.0014357034298073915\n",
      "Epoch: 61 Loss: 0.0014360960404802528\n",
      "Epoch: 62 Loss: 0.0014329813679473267\n",
      "Epoch: 63 Loss: 0.001438539074216452\n",
      "Epoch: 64 Loss: 0.0014370497536017662\n",
      "Epoch: 65 Loss: 0.00143305888114911\n",
      "Epoch: 66 Loss: 0.0014411950509788261\n",
      "Epoch: 67 Loss: 0.0014348983014416364\n",
      "Epoch: 68 Loss: 0.0014331629370442694\n",
      "Epoch: 69 Loss: 0.0014410600544781322\n",
      "Epoch: 70 Loss: 0.0014345508857837154\n",
      "Epoch: 71 Loss: 0.0014348217066273922\n",
      "Epoch: 72 Loss: 0.0014343289399726524\n",
      "Epoch: 73 Loss: 0.0014392422292278046\n",
      "Epoch: 74 Loss: 0.0014356400804697638\n",
      "Epoch: 75 Loss: 0.0014349689720095033\n",
      "Epoch: 76 Loss: 0.0014370625528196495\n",
      "Epoch: 77 Loss: 0.0014356600974376004\n",
      "Epoch: 78 Loss: 0.0014335281707139479\n",
      "Epoch: 79 Loss: 0.0014350087084393534\n",
      "Epoch: 80 Loss: 0.001443161577400234\n",
      "Epoch: 81 Loss: 0.0014440259740998347\n",
      "Epoch: 82 Loss: 0.001437911104100446\n",
      "Epoch: 83 Loss: 0.0014365482363953358\n",
      "Epoch: 84 Loss: 0.0014339614685417877\n",
      "Epoch: 85 Loss: 0.0014363448782306579\n",
      "Epoch: 86 Loss: 0.0014359783898625108\n",
      "Epoch: 87 Loss: 0.001442803664960795\n",
      "Epoch: 88 Loss: 0.0014435489364485773\n",
      "Epoch: 89 Loss: 0.001447073565537317\n",
      "Epoch: 90 Loss: 0.0014349744241270754\n",
      "Epoch: 91 Loss: 0.0014440372275809448\n",
      "Epoch: 92 Loss: 0.0014376737979344195\n",
      "Epoch: 93 Loss: 0.001434669189620763\n",
      "Epoch: 94 Loss: 0.0014487180354384084\n",
      "Epoch: 95 Loss: 0.0014358863570830887\n",
      "Epoch: 96 Loss: 0.0014370328345749942\n",
      "Epoch: 97 Loss: 0.0014387334942714209\n",
      "Epoch: 98 Loss: 0.0014339576074336139\n",
      "Epoch: 99 Loss: 0.00145600619725883\n",
      "Epoch: 100 Loss: 0.0014723401594286163\n",
      "Epoch: 101 Loss: 0.0014456830297907193\n",
      "Epoch: 102 Loss: 0.0014367378316819668\n",
      "Epoch: 103 Loss: 0.0014446810689858263\n",
      "Epoch: 104 Loss: 0.0014381212725614507\n",
      "Epoch: 105 Loss: 0.0014266675959030788\n",
      "Epoch: 106 Loss: 0.0014327007617490988\n",
      "Epoch: 107 Loss: 0.0014360553727278279\n",
      "Epoch: 108 Loss: 0.0014708112187994022\n",
      "Epoch: 109 Loss: 0.00143963815126982\n",
      "Epoch: 110 Loss: 0.001432091211124013\n",
      "Epoch: 111 Loss: 0.001467317860159609\n",
      "Epoch: 112 Loss: 0.0014362183994510109\n",
      "Epoch: 113 Loss: 0.0014134412437366943\n",
      "Epoch: 114 Loss: 0.0014322239504609671\n",
      "Epoch: 115 Loss: 0.0014386800790412559\n",
      "Epoch: 116 Loss: 0.0014274307371427615\n",
      "Epoch: 117 Loss: 0.0014211477892887262\n",
      "Epoch: 118 Loss: 0.0014056125397069587\n",
      "Epoch: 119 Loss: 0.0013963080192398694\n",
      "Epoch: 120 Loss: 0.0013877552895185847\n",
      "Epoch: 121 Loss: 0.0013892004175836013\n",
      "Epoch: 122 Loss: 0.001445911158548875\n",
      "Epoch: 123 Loss: 0.0013886513964583476\n",
      "Epoch: 124 Loss: 0.001388672545241813\n",
      "Epoch: 125 Loss: 0.001386428187187347\n",
      "Epoch: 126 Loss: 0.0014054636057052347\n",
      "Epoch: 127 Loss: 0.0013846733296910922\n",
      "Epoch: 128 Loss: 0.0013824274841075142\n",
      "Epoch: 129 Loss: 0.0013768173392034238\n",
      "Epoch: 130 Loss: 0.001377722811109076\n",
      "Epoch: 131 Loss: 0.0013641577097587287\n",
      "Epoch: 132 Loss: 0.0013553575069333117\n",
      "Epoch: 133 Loss: 0.0013869673776854244\n",
      "Epoch: 134 Loss: 0.0013474708216057883\n",
      "Epoch: 135 Loss: 0.0013648414428138898\n",
      "Epoch: 136 Loss: 0.0013590209354232582\n",
      "Epoch: 137 Loss: 0.0013870029878388676\n",
      "Epoch: 138 Loss: 0.0013856720438020097\n",
      "Epoch: 139 Loss: 0.0013644841706587209\n",
      "Epoch: 140 Loss: 0.0013879032275225553\n",
      "Epoch: 141 Loss: 0.0013887964305467904\n",
      "Epoch: 142 Loss: 0.0013882413558248016\n",
      "Epoch: 143 Loss: 0.0013739267273599075\n",
      "Epoch: 144 Loss: 0.0013472185948760146\n",
      "Epoch: 145 Loss: 0.0013470150103482108\n",
      "Epoch: 146 Loss: 0.0013344310249926315\n",
      "Epoch: 147 Loss: 0.0013286906372134883\n",
      "Epoch: 148 Loss: 0.0013260923377755615\n",
      "Epoch: 149 Loss: 0.0013293764269393352\n",
      "Epoch: 150 Loss: 0.0013243923895061016\n",
      "Epoch: 151 Loss: 0.0013317085621464583\n",
      "Epoch: 152 Loss: 0.0013253069806119634\n",
      "Epoch: 153 Loss: 0.0013293039713365336\n",
      "Epoch: 154 Loss: 0.0013376729330047965\n",
      "Epoch: 155 Loss: 0.0013333891919400129\n",
      "Epoch: 156 Loss: 0.0013256929491439627\n",
      "Epoch: 157 Loss: 0.0013891570205386314\n",
      "Epoch: 158 Loss: 0.0013472953125730986\n",
      "Epoch: 159 Loss: 0.0013213569958073397\n",
      "Epoch: 160 Loss: 0.0013205582896868389\n",
      "Epoch: 161 Loss: 0.0013225578715921277\n",
      "Epoch: 162 Loss: 0.0013191279787052837\n",
      "Epoch: 163 Loss: 0.0013236966656727924\n",
      "Epoch: 164 Loss: 0.0013243599160988298\n",
      "Epoch: 165 Loss: 0.0013257986995288068\n",
      "Epoch: 166 Loss: 0.0013229515688079926\n",
      "Epoch: 167 Loss: 0.0013182180118747056\n",
      "Epoch: 168 Loss: 0.0013225228952554364\n",
      "Epoch: 169 Loss: 0.0013244982951113747\n",
      "Epoch: 170 Loss: 0.0013254111658574806\n",
      "Epoch: 171 Loss: 0.0013322682223386234\n",
      "Epoch: 172 Loss: 0.0013350826726915936\n",
      "Epoch: 173 Loss: 0.0013200514949858189\n",
      "Epoch: 174 Loss: 0.001321691372949216\n",
      "Epoch: 175 Loss: 0.001315674440573073\n",
      "Epoch: 176 Loss: 0.0013301127538498905\n",
      "Epoch: 177 Loss: 0.0013334807849282192\n",
      "Epoch: 178 Loss: 0.0013286530091944668\n",
      "Epoch: 179 Loss: 0.0013201729614391094\n",
      "Epoch: 180 Loss: 0.0013224773445270127\n",
      "Epoch: 181 Loss: 0.0013384266381358935\n",
      "Epoch: 182 Loss: 0.0013882523441376786\n",
      "Epoch: 183 Loss: 0.0013611790814643933\n",
      "Epoch: 184 Loss: 0.0013159822038788763\n",
      "Epoch: 185 Loss: 0.0013401296584763462\n",
      "Epoch: 186 Loss: 0.0013211023308233255\n",
      "Epoch: 187 Loss: 0.001418448570701811\n",
      "Epoch: 188 Loss: 0.00127431300158302\n",
      "Epoch: 189 Loss: 0.001183917202676336\n",
      "Epoch: 190 Loss: 0.0011696335018819405\n",
      "Epoch: 191 Loss: 0.0011767842095448738\n",
      "Epoch: 192 Loss: 0.001203825238109049\n",
      "Epoch: 193 Loss: 0.0011774012172180745\n",
      "Epoch: 194 Loss: 0.0011686849045670694\n",
      "Epoch: 195 Loss: 0.0011658373275875216\n",
      "Epoch: 196 Loss: 0.0011544959787796768\n",
      "Epoch: 197 Loss: 0.0011437107200941278\n",
      "Epoch: 198 Loss: 0.0011561950798042947\n",
      "Epoch: 199 Loss: 0.0011052248171634143\n",
      "Epoch: 200 Loss: 0.001089654956659716\n",
      "Epoch: 201 Loss: 0.0010714161584878133\n",
      "Epoch: 202 Loss: 0.0010795855551906344\n",
      "Epoch: 203 Loss: 0.0010675096968447582\n",
      "Epoch: 204 Loss: 0.001065312561372088\n",
      "Epoch: 205 Loss: 0.0010627599969868446\n",
      "Epoch: 206 Loss: 0.0010507709197958724\n",
      "Epoch: 207 Loss: 0.0010902659139699405\n",
      "Epoch: 208 Loss: 0.0011544439993384811\n",
      "Epoch: 209 Loss: 0.0012005215976387262\n",
      "Epoch: 210 Loss: 0.0011639889659515272\n",
      "Epoch: 211 Loss: 0.001065787810754652\n",
      "Epoch: 212 Loss: 0.001057980924896482\n",
      "Epoch: 213 Loss: 0.0010634763166308403\n",
      "Epoch: 214 Loss: 0.0010863494341416906\n",
      "Epoch: 215 Loss: 0.0010733783419709653\n",
      "Epoch: 216 Loss: 0.0010725729258006646\n",
      "Epoch: 217 Loss: 0.0010515208672990815\n",
      "Epoch: 218 Loss: 0.0010856056016766364\n",
      "Epoch: 219 Loss: 0.0010914592680314349\n",
      "Epoch: 220 Loss: 0.001052634809942295\n",
      "Epoch: 221 Loss: 0.0010909802060470814\n",
      "Epoch: 222 Loss: 0.0010742181200637585\n",
      "Epoch: 223 Loss: 0.0010473123984411359\n",
      "Epoch: 224 Loss: 0.0010370109683006173\n",
      "Epoch: 225 Loss: 0.0010398644889290961\n",
      "Epoch: 226 Loss: 0.001054220661495088\n",
      "Epoch: 227 Loss: 0.0010414213724693076\n",
      "Epoch: 228 Loss: 0.0010754000945275442\n",
      "Epoch: 229 Loss: 0.0010254278992457937\n",
      "Epoch: 230 Loss: 0.001018205039953399\n",
      "Epoch: 231 Loss: 0.0010089079086254868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 232 Loss: 0.0010274108563963738\n",
      "Epoch: 233 Loss: 0.0009983008785638958\n",
      "Epoch: 234 Loss: 0.000992006641657402\n",
      "Epoch: 235 Loss: 0.0010556866246689525\n",
      "Epoch: 236 Loss: 0.0010301431385515672\n",
      "Epoch: 237 Loss: 0.0010617154572779934\n",
      "Epoch: 238 Loss: 0.0010134587549449254\n",
      "Epoch: 239 Loss: 0.0010338466316978964\n",
      "Epoch: 240 Loss: 0.0009897133018562777\n",
      "Epoch: 241 Loss: 0.0009849896499266226\n",
      "Epoch: 242 Loss: 0.001047258949256502\n",
      "Epoch: 243 Loss: 0.001081418161953075\n",
      "Epoch: 244 Loss: 0.001009420634040402\n",
      "Epoch: 245 Loss: 0.0009764033052811606\n",
      "Epoch: 246 Loss: 0.0009595119465504669\n",
      "Epoch: 247 Loss: 0.0009657514423856305\n",
      "Epoch: 248 Loss: 0.0009385020789017694\n",
      "Epoch: 249 Loss: 0.0009168698937476923\n",
      "Epoch: 250 Loss: 0.0009668883010615698\n",
      "Epoch: 251 Loss: 0.0008982579877030932\n",
      "Epoch: 252 Loss: 0.0008747180885014435\n",
      "Epoch: 253 Loss: 0.0009455379913561046\n",
      "Epoch: 254 Loss: 0.0009270661701318912\n",
      "Epoch: 255 Loss: 0.0008310917958927652\n",
      "Epoch: 256 Loss: 0.0008143720139438907\n",
      "Epoch: 257 Loss: 0.0008009459351241174\n",
      "Epoch: 258 Loss: 0.0008820540226426803\n",
      "Epoch: 259 Loss: 0.0009504843587314503\n",
      "Epoch: 260 Loss: 0.0008066472801147029\n",
      "Epoch: 261 Loss: 0.0007932155842556515\n",
      "Epoch: 262 Loss: 0.0007993069367431519\n",
      "Epoch: 263 Loss: 0.0007974041525610826\n",
      "Epoch: 264 Loss: 0.000817942995733271\n",
      "Epoch: 265 Loss: 0.0008606837557939192\n",
      "Epoch: 266 Loss: 0.0008808238473850199\n",
      "Epoch: 267 Loss: 0.0008657523948285314\n",
      "Epoch: 268 Loss: 0.0008551473244248579\n",
      "Epoch: 269 Loss: 0.0008046125909054859\n",
      "Epoch: 270 Loss: 0.0007874257668542365\n",
      "Epoch: 271 Loss: 0.0008055608599938245\n",
      "Epoch: 272 Loss: 0.0007860227439474935\n",
      "Epoch: 273 Loss: 0.0007863666622951212\n",
      "Epoch: 274 Loss: 0.0007830262200311861\n",
      "Epoch: 275 Loss: 0.0007824341810191981\n",
      "Epoch: 276 Loss: 0.000782606728737139\n",
      "Epoch: 277 Loss: 0.0007778725088832693\n",
      "Epoch: 278 Loss: 0.0007913612579512927\n",
      "Epoch: 279 Loss: 0.0007670348568353802\n",
      "Epoch: 280 Loss: 0.0008148927332109048\n",
      "Epoch: 281 Loss: 0.0007855819389482753\n",
      "Epoch: 282 Loss: 0.000775394071954199\n",
      "Epoch: 283 Loss: 0.0007802184490073058\n",
      "Epoch: 284 Loss: 0.000753469304375661\n",
      "Epoch: 285 Loss: 0.0007475913864456945\n",
      "Epoch: 286 Loss: 0.0007350410435012438\n",
      "Epoch: 287 Loss: 0.0007185438880696893\n",
      "Epoch: 288 Loss: 0.0007163344012547492\n",
      "Epoch: 289 Loss: 0.000717663338744185\n",
      "Epoch: 290 Loss: 0.0008032836647342063\n",
      "Epoch: 291 Loss: 0.0007463162603218936\n",
      "Epoch: 292 Loss: 0.0008106595616684192\n",
      "Epoch: 293 Loss: 0.0007701616947694371\n",
      "Epoch: 294 Loss: 0.0006950105339961334\n",
      "Epoch: 295 Loss: 0.0006853023587609641\n",
      "Epoch: 296 Loss: 0.0008064410422876892\n",
      "Epoch: 297 Loss: 0.0008017000881308275\n",
      "Epoch: 298 Loss: 0.0007586213194170139\n",
      "Epoch: 299 Loss: 0.0006855194935471647\n",
      "Epoch: 300 Loss: 0.0006875245380797423\n",
      "Epoch: 301 Loss: 0.0007122846226492482\n",
      "Epoch: 302 Loss: 0.0008176701893616053\n",
      "Epoch: 303 Loss: 0.0007669172644252992\n",
      "Epoch: 304 Loss: 0.0007416749463623597\n",
      "Epoch: 305 Loss: 0.0007287115117328034\n",
      "Epoch: 306 Loss: 0.0006507344239960528\n",
      "Epoch: 307 Loss: 0.0007714382243446178\n",
      "Epoch: 308 Loss: 0.0006505428076100846\n",
      "Epoch: 309 Loss: 0.0006980330541005565\n",
      "Epoch: 310 Loss: 0.0007538768889692923\n",
      "Epoch: 311 Loss: 0.0006317620928813186\n",
      "Epoch: 312 Loss: 0.0006495608783249432\n",
      "Epoch: 313 Loss: 0.0006094692847303426\n",
      "Epoch: 314 Loss: 0.0005993952153302315\n",
      "Epoch: 315 Loss: 0.0005943961312166519\n",
      "Epoch: 316 Loss: 0.0005883369699909559\n",
      "Epoch: 317 Loss: 0.000607653786169572\n",
      "Epoch: 318 Loss: 0.0007054791236037596\n",
      "Epoch: 319 Loss: 0.0006801410054322332\n",
      "Epoch: 320 Loss: 0.0007577867719293055\n",
      "Epoch: 321 Loss: 0.0006673664546623412\n",
      "Epoch: 322 Loss: 0.0006153228621567703\n",
      "Epoch: 323 Loss: 0.0005987736868620333\n",
      "Epoch: 324 Loss: 0.0006671950314840716\n",
      "Epoch: 325 Loss: 0.000661285141379469\n",
      "Epoch: 326 Loss: 0.00060565704997215\n",
      "Epoch: 327 Loss: 0.0006088238765692546\n",
      "Epoch: 328 Loss: 0.0006141808472522018\n",
      "Epoch: 329 Loss: 0.0005831086212613931\n",
      "Epoch: 330 Loss: 0.0005784030589792463\n",
      "Epoch: 331 Loss: 0.0005865025870864176\n",
      "Epoch: 332 Loss: 0.0006924734981213179\n",
      "Epoch: 333 Loss: 0.0006382651594726162\n",
      "Epoch: 334 Loss: 0.0006453688838519156\n",
      "Epoch: 335 Loss: 0.0007128633442334831\n",
      "Epoch: 336 Loss: 0.0006158190258752762\n",
      "Epoch: 337 Loss: 0.0006073975576631104\n",
      "Epoch: 338 Loss: 0.0005745001302178329\n",
      "Epoch: 339 Loss: 0.0006609414130151789\n",
      "Epoch: 340 Loss: 0.0005999145489315399\n",
      "Epoch: 341 Loss: 0.0005795557258857621\n",
      "Epoch: 342 Loss: 0.000594928820242381\n",
      "Epoch: 343 Loss: 0.0006603242751831809\n",
      "Epoch: 344 Loss: 0.0006130107115798941\n",
      "Epoch: 345 Loss: 0.000584844701936365\n",
      "Epoch: 346 Loss: 0.0005781529325759038\n",
      "Epoch: 347 Loss: 0.0005854213296616864\n",
      "Epoch: 348 Loss: 0.0005910567682197628\n",
      "Epoch: 349 Loss: 0.000613491062217185\n",
      "Epoch: 350 Loss: 0.0006657669129910775\n",
      "Epoch: 351 Loss: 0.0005865897110197693\n",
      "Epoch: 352 Loss: 0.0005829614124700634\n",
      "Epoch: 353 Loss: 0.000628791427718372\n",
      "Epoch: 354 Loss: 0.0005927991264292763\n",
      "Epoch: 355 Loss: 0.0006047912902431563\n",
      "Epoch: 356 Loss: 0.0005703991919290274\n",
      "Epoch: 357 Loss: 0.0005650966227727218\n",
      "Epoch: 358 Loss: 0.0006104808740524782\n",
      "Epoch: 359 Loss: 0.0009273120441422281\n",
      "Epoch: 360 Loss: 0.0006603356644821664\n",
      "Epoch: 361 Loss: 0.0005949135552833064\n",
      "Epoch: 362 Loss: 0.0005787617054819646\n",
      "Epoch: 363 Loss: 0.0005680927974430637\n",
      "Epoch: 364 Loss: 0.0005865955317858607\n",
      "Epoch: 365 Loss: 0.0005855544053095704\n",
      "Epoch: 366 Loss: 0.0006162249289142588\n",
      "Epoch: 367 Loss: 0.0006764415625689758\n",
      "Epoch: 368 Loss: 0.0005749153966058253\n",
      "Epoch: 369 Loss: 0.0005703752895998252\n",
      "Epoch: 370 Loss: 0.0005576057875360777\n",
      "Epoch: 371 Loss: 0.0005900756595009523\n",
      "Epoch: 372 Loss: 0.0006944110969521312\n",
      "Epoch: 373 Loss: 0.0005751479911850765\n",
      "Epoch: 374 Loss: 0.0005761324751397802\n",
      "Epoch: 375 Loss: 0.0005632502725347877\n",
      "Epoch: 376 Loss: 0.0005777630738319001\n",
      "Epoch: 377 Loss: 0.0005697406886611134\n",
      "Epoch: 378 Loss: 0.0005578808437955255\n",
      "Epoch: 379 Loss: 0.000550132144578836\n",
      "Epoch: 380 Loss: 0.0006407667436481764\n",
      "Epoch: 381 Loss: 0.0006626326859178436\n",
      "Epoch: 382 Loss: 0.000610509007755253\n",
      "Epoch: 383 Loss: 0.0005437613387281696\n",
      "Epoch: 384 Loss: 0.0005572578552851661\n",
      "Epoch: 385 Loss: 0.0006218161433935165\n",
      "Epoch: 386 Loss: 0.0005725162708485085\n",
      "Epoch: 387 Loss: 0.0006939693460784232\n",
      "Epoch: 388 Loss: 0.0005574124693844675\n",
      "Epoch: 389 Loss: 0.0005541054367010171\n",
      "Epoch: 390 Loss: 0.0005526443047630084\n",
      "Epoch: 391 Loss: 0.0005754388726523353\n",
      "Epoch: 392 Loss: 0.0006100427029499164\n",
      "Epoch: 393 Loss: 0.0005653120848971108\n",
      "Epoch: 394 Loss: 0.0006301424695670398\n",
      "Epoch: 395 Loss: 0.0005355942751824235\n",
      "Epoch: 396 Loss: 0.0005445186753705558\n",
      "Epoch: 397 Loss: 0.0005436564420556857\n",
      "Epoch: 398 Loss: 0.0005740009704216694\n",
      "Epoch: 399 Loss: 0.0005580099645562263\n",
      "Epoch: 400 Loss: 0.0006433693553036493\n",
      "Epoch: 401 Loss: 0.0006069089997456305\n",
      "Epoch: 402 Loss: 0.0006968251368056776\n",
      "Epoch: 403 Loss: 0.0005301146965292799\n",
      "Epoch: 404 Loss: 0.0005237343721091747\n",
      "Epoch: 405 Loss: 0.0005221133275578419\n",
      "Epoch: 406 Loss: 0.0005364865549684813\n",
      "Epoch: 407 Loss: 0.0005580607329546991\n",
      "Epoch: 408 Loss: 0.0005343842656455106\n",
      "Epoch: 409 Loss: 0.0005771458559643684\n",
      "Epoch: 410 Loss: 0.0005365459526526845\n",
      "Epoch: 411 Loss: 0.0005227128163419871\n",
      "Epoch: 412 Loss: 0.0005253012026918845\n",
      "Epoch: 413 Loss: 0.000523863009422914\n",
      "Epoch: 414 Loss: 0.0005471977492561564\n",
      "Epoch: 415 Loss: 0.0005628849871249662\n",
      "Epoch: 416 Loss: 0.00058793995089622\n",
      "Epoch: 417 Loss: 0.0005609141112977846\n",
      "Epoch: 418 Loss: 0.0005809555619230701\n",
      "Epoch: 419 Loss: 0.0005343365450648384\n",
      "Epoch: 420 Loss: 0.0005687137389840144\n",
      "Epoch: 421 Loss: 0.0005512860807357356\n",
      "Epoch: 422 Loss: 0.0005633396083592541\n",
      "Epoch: 423 Loss: 0.0005268421955406666\n",
      "Epoch: 424 Loss: 0.0005402517272159457\n",
      "Epoch: 425 Loss: 0.0005483933459294753\n",
      "Epoch: 426 Loss: 0.00048098079019433097\n",
      "Epoch: 427 Loss: 0.00048192537926499627\n",
      "Epoch: 428 Loss: 0.0005169880799561118\n",
      "Epoch: 429 Loss: 0.0004914714130184924\n",
      "Epoch: 430 Loss: 0.0004817054820402215\n",
      "Epoch: 431 Loss: 0.0005207563679303146\n",
      "Epoch: 432 Loss: 0.0006103998425209688\n",
      "Epoch: 433 Loss: 0.0005445832955754466\n",
      "Epoch: 434 Loss: 0.0005042705096356156\n",
      "Epoch: 435 Loss: 0.0004925679548048518\n",
      "Epoch: 436 Loss: 0.00046909218912737235\n",
      "Epoch: 437 Loss: 0.00047616447005162225\n",
      "Epoch: 438 Loss: 0.0005119837215817017\n",
      "Epoch: 439 Loss: 0.0005103161965962499\n",
      "Epoch: 440 Loss: 0.0005554682154777563\n",
      "Epoch: 441 Loss: 0.0005179849799282642\n",
      "Epoch: 442 Loss: 0.0005055028725311988\n",
      "Epoch: 443 Loss: 0.0005116838324789165\n",
      "Epoch: 444 Loss: 0.0005649596148739672\n",
      "Epoch: 445 Loss: 0.0005205371497949171\n",
      "Epoch: 446 Loss: 0.0005255552273916288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 447 Loss: 0.00047351711772434\n",
      "Epoch: 448 Loss: 0.0004927858260796509\n",
      "Epoch: 449 Loss: 0.0005284090907985552\n",
      "Epoch: 450 Loss: 0.0005374652084558167\n",
      "Epoch: 451 Loss: 0.0005009089460751662\n",
      "Epoch: 452 Loss: 0.00048301436375671375\n",
      "Epoch: 453 Loss: 0.000463651054208943\n",
      "Epoch: 454 Loss: 0.000522301233205427\n",
      "Epoch: 455 Loss: 0.0006579363366149159\n",
      "Epoch: 456 Loss: 0.0005389596190070733\n",
      "Epoch: 457 Loss: 0.0005061087010997451\n",
      "Epoch: 458 Loss: 0.0005563853887401314\n",
      "Epoch: 459 Loss: 0.000554494483771527\n",
      "Epoch: 460 Loss: 0.00047850931231449876\n",
      "Epoch: 461 Loss: 0.000502747651706967\n",
      "Epoch: 462 Loss: 0.0004825830110348761\n",
      "Epoch: 463 Loss: 0.0004779156847184317\n",
      "Epoch: 464 Loss: 0.0005539507449915012\n",
      "Epoch: 465 Loss: 0.0004837093519098643\n",
      "Epoch: 466 Loss: 0.0004656783017304002\n",
      "Epoch: 467 Loss: 0.0004596986432766749\n",
      "Epoch: 468 Loss: 0.0004950357364982159\n",
      "Epoch: 469 Loss: 0.0005480162524488858\n",
      "Epoch: 470 Loss: 0.0005025819765352127\n",
      "Epoch: 471 Loss: 0.0004617454962701433\n",
      "Epoch: 472 Loss: 0.0004680577403632924\n",
      "Epoch: 473 Loss: 0.0005331747460230771\n",
      "Epoch: 474 Loss: 0.0005130749971916279\n",
      "Epoch: 475 Loss: 0.00047684125375882204\n",
      "Epoch: 476 Loss: 0.0004775521067333304\n",
      "Epoch: 477 Loss: 0.00045399262227066275\n",
      "Epoch: 478 Loss: 0.0004389449895825237\n",
      "Epoch: 479 Loss: 0.0004486247893914373\n",
      "Epoch: 480 Loss: 0.00044523528291999054\n",
      "Epoch: 481 Loss: 0.00044280993946207065\n",
      "Epoch: 482 Loss: 0.0004848435443515579\n",
      "Epoch: 483 Loss: 0.0004926014403786717\n",
      "Epoch: 484 Loss: 0.00048318924867392826\n",
      "Epoch: 485 Loss: 0.0004577517894277763\n",
      "Epoch: 486 Loss: 0.0004939051965872446\n",
      "Epoch: 487 Loss: 0.00048315683994183526\n",
      "Epoch: 488 Loss: 0.00045073226404686767\n",
      "Epoch: 489 Loss: 0.00045326483733434643\n",
      "Epoch: 490 Loss: 0.00044337585859466344\n",
      "Epoch: 491 Loss: 0.0004340701561886817\n",
      "Epoch: 492 Loss: 0.0005252787992099507\n",
      "Epoch: 493 Loss: 0.0005269287034429403\n",
      "Epoch: 494 Loss: 0.0006269745677450879\n",
      "Epoch: 495 Loss: 0.0005009169884336492\n",
      "Epoch: 496 Loss: 0.0004618225664469517\n",
      "Epoch: 497 Loss: 0.0004665846039036599\n",
      "Epoch: 498 Loss: 0.00045543550125633675\n",
      "Epoch: 499 Loss: 0.0006021301071288892\n",
      "Epoch: 500 Loss: 0.0004904982924927026\n",
      "Predictions...\n",
      "Prediction Shape: (129034, 3)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    session.run(local_init)\n",
    "\n",
    "    num_batches = int(matrix.shape[0] / batch_size)\n",
    "    matrix = np.array_split(matrix, num_batches)\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        avg_cost = 0\n",
    "\n",
    "        for batch in matrix:\n",
    "            _, l = session.run([optimizer, loss], feed_dict={X: batch})\n",
    "            avg_cost += l\n",
    "\n",
    "        avg_cost /= num_batches\n",
    "\n",
    "        print(\"Epoch: {} Loss: {}\".format(i + 1, avg_cost))\n",
    "\n",
    "        # if i % display_step == 0 or i == 1:\n",
    "        #     print('Step %i: Minibatch Loss: %f' % (i, l))\n",
    "\n",
    "    print(\"Predictions...\")\n",
    "\n",
    "    matrix = np.concatenate(matrix, axis=0)\n",
    "\n",
    "    preds = session.run(decoder_op, feed_dict={X: matrix})\n",
    "\n",
    "    # print(matrix)\n",
    "    # print(preds)\n",
    "    \n",
    "    predictions = predictions.append(pd.DataFrame(preds))\n",
    "\n",
    "    predictions = predictions.stack().reset_index(name='rating')\n",
    "    predictions.columns = ['user', 'book', 'rating']\n",
    "    predictions['user'] = predictions['user'].map(lambda value: users[value])\n",
    "    predictions['book'] = predictions['book'].map(lambda value: books[value])\n",
    "\n",
    "    print(f'Prediction Shape: {predictions.shape}')\n",
    "    if debug:\n",
    "        print(predictions)\n",
    "        \n",
    "    \n",
    "    keys = ['user', 'book']\n",
    "    i1 = predictions.set_index(keys).index\n",
    "    i2 = df.set_index(keys).index\n",
    "\n",
    "    recs = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Out CSV\n",
    "============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Min: 0.0\n"
     ]
    }
   ],
   "source": [
    "recs['user'] = recs['user'] * user_range + user_min\n",
    "recs['book'] = recs['book'] * book_range + book_min\n",
    "print(f'User Min: {user_min}')\n",
    "recs = recs.sort_values(['user', 'rating'], ascending=[True, False])\n",
    "recs.to_csv('prediction.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Out SQL INSERT\n",
    "=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of recommendations to save for each user\n",
    "save_X_recommendations = 100\n",
    "\n",
    "with open('../../db/create_tables_insert_data/Recommendations.sql', 'w') as file:\n",
    "    file.write('''\n",
    "-- Created uing the Jupyter notebook\n",
    "\n",
    "\\\\set ON_ERROR_STOP on\n",
    "SET CLIENT_ENCODING TO 'utf8';\n",
    "\n",
    "drop table if exists \"Recommendations\";\n",
    "drop index if exists \"Recommendations_pkey\";\n",
    "drop index if exists \"Recommendations_book_id_fk\";\n",
    "drop index if exists \"Recommendations_user_id_fk\";\n",
    "\n",
    "CREATE TABLE public.\"Recommendations\" (\n",
    "    user_id integer NOT NULL,\n",
    "    book_id integer NOT NULL,\n",
    "    likelihood numeric NOT NULL\n",
    ");\\n\\n\\n''')\n",
    "    for x in recs.groupby('user').head(save_X_recommendations).itertuples():\n",
    "        file.write('INSERT INTO public.\"Recommendations\" (user_id, book_id, likelihood) VALUES '+\n",
    "                   f'({int(round(x.user))}, {int(round(x.book))}, {x.rating});\\n')\n",
    "    file.write('''\n",
    "ALTER TABLE ONLY public.\"Recommendations\"\n",
    "    ADD CONSTRAINT \"Recommendations_pkey\" PRIMARY KEY (user_id, book_id);\n",
    "ALTER TABLE ONLY public.\"Recommendations\"\n",
    "    ADD CONSTRAINT Recommendations_book_id_fk FOREIGN KEY (book_id) REFERENCES public.\"Books\"(book_id);\n",
    "ALTER TABLE ONLY public.\"Recommendations\"\n",
    "    ADD CONSTRAINT Recommendations_user_id_fk FOREIGN KEY (user_id) REFERENCES public.\"Users\"(user_id);\n",
    "\n",
    "GRANT ALL ON TABLE public.\"Recommendations\" TO ece651_ml;\n",
    "GRANT ALL ON TABLE public.\"Recommendations\" TO ece651_web;\n",
    "GRANT ALL ON TABLE public.\"Recommendations\" TO ece651_scraper;\n",
    "\n",
    "\\\\unset ON_ERROR_STOP\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    recs.loc[recs['user'] == 2380]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(866,)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs.loc[recs['user'] == 2380]['book'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find 677 for user 2380\n"
     ]
    }
   ],
   "source": [
    "user_2380_top = recs.loc[recs['user'] == 2380].head(20)\n",
    "\n",
    "expected_2380_book_ids = [382,670,662,375,677];\n",
    "for x in expected_2380_book_ids:\n",
    "    if x not in user_2380_top['book'].values.round(): \n",
    "        print(f'Couldn\\'t find {x} for user 2380')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    recs.loc[recs['user'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(866,)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs.loc[recs['user'] == 1]['book'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find 1374 for user 1\n",
      "Couldn't find 1420 for user 1\n",
      "Couldn't find 1526 for user 1\n",
      "Couldn't find 1384 for user 1\n",
      "Couldn't find 1385 for user 1\n"
     ]
    }
   ],
   "source": [
    "user_1_top = recs.loc[recs['user'] == 1].head(20)\n",
    "\n",
    "expected_1_book_ids = [1387,1374,1420,1526,1308,1384,1210,1385];\n",
    "for x in expected_1_book_ids:\n",
    "    if x not in user_1_top['book'].values.round(): \n",
    "        print(f'Couldn\\'t find {x} for user 1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
