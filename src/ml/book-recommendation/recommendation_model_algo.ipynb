{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Recommender System in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "epochs = 10\n",
    "display_step = 10\n",
    "\n",
    "learning_rate = 0.3\n",
    "\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Dataset and splitting it in a training set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USERS: 147 BOOKS: 40\n",
      "   user  book  rating\n",
      "0  2292   360       5\n",
      "1  2293   360       5\n",
      "2  2294   360       5\n",
      "3  2297   655       4\n",
      "4  2295   360       5\n"
     ]
    }
   ],
   "source": [
    "sql = 'SELECT user_id, book_id, rating, date_created FROM public.\"Reviews\"'\n",
    "\n",
    "engine = create_engine('postgresql://ece651_ml:TVL3MV0mguz0DOhLbbm2@localhost:5432/ece651')\n",
    "\n",
    "# Reading dataset\n",
    "\n",
    "df = pd.pandas.read_sql(sql, engine)\n",
    "\n",
    "y = df.date_created\n",
    "df = df.drop('date_created', axis=1)\n",
    "\n",
    "df.columns = ['user', 'book', 'rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)\n",
    "\n",
    "train_data = X_train\n",
    "test_data = X_test\n",
    "\n",
    "num_books = df.book.nunique()\n",
    "num_users = df.user.nunique()\n",
    "\n",
    "print(\"USERS: {} BOOKS: {}\".format(num_users, num_books))\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training set with three columns: user, book and ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize in [0, 1]\n",
    "\n",
    "u = df['user'].values.astype(float)\n",
    "\n",
    "user_min = u.min()\n",
    "user_range = u.max() - u.min()\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(u.reshape(-1,1))\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "df['user'] = df_normalized\n",
    "\n",
    "\n",
    "b = df['book'].values.astype(float)\n",
    "\n",
    "book_min = b.min()\n",
    "book_range = b.max() - b.min()\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(b.reshape(-1,1))\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "df['book'] = df_normalized\n",
    "\n",
    "r = df['rating'].values.astype(float)\n",
    "\n",
    "rating_min = r.min()\n",
    "rating_range = r.max() - r.min()\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(r.reshape(-1,1))\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "df['rating'] = df_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert DataFrame in user-item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = df.pivot(index='user', columns='book', values='rating')\n",
    "matrix.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users and items ordered as they are in matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (147, 40)\n"
     ]
    }
   ],
   "source": [
    "users = matrix.index.tolist()\n",
    "books = matrix.columns.tolist()\n",
    "\n",
    "matrix = matrix.values\n",
    "\n",
    "print(\"Matrix shape: {}\".format(matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = num_books   # num of items\n",
    "num_hidden_1 = 10       # 1st layer num features\n",
    "num_hidden_2 = 5        # 2nd layer num features (the latent dim)\n",
    "\n",
    "X = tf.placeholder(tf.float64, [None, num_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = decoder_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targets are the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer, minimize the squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "predictions = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_x = tf.placeholder(tf.int32, )\n",
    "eval_y = tf.placeholder(tf.int32, )\n",
    "pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.32607042193412783\n",
      "Epoch: 2 Loss: 0.32056357264518737\n",
      "Epoch: 3 Loss: 0.3134516477584839\n",
      "Epoch: 4 Loss: 0.30433675050735476\n",
      "Epoch: 5 Loss: 0.2928016185760498\n",
      "Epoch: 6 Loss: 0.27848378419876096\n",
      "Epoch: 7 Loss: 0.26117937862873075\n",
      "Epoch: 8 Loss: 0.24092041552066804\n",
      "Epoch: 9 Loss: 0.21784095466136932\n",
      "Epoch: 10 Loss: 0.19158402383327483\n",
      "Predictions...\n",
      "      user      book    rating\n",
      "0      0.0  0.000000  0.240523\n",
      "1      0.0  0.300000  0.280023\n",
      "2      0.0  0.316667  0.937551\n",
      "3      0.0  0.319231  0.128938\n",
      "4      0.0  0.323077  0.707076\n",
      "5      0.0  0.325641  0.847956\n",
      "6      0.0  0.328205  0.037666\n",
      "7      0.0  0.379487  0.216074\n",
      "8      0.0  0.678205  0.186799\n",
      "9      0.0  0.679487  0.989310\n",
      "10     0.0  0.687179  0.182303\n",
      "11     0.0  0.688462  0.081031\n",
      "12     0.0  0.689744  0.168084\n",
      "13     0.0  0.697436  0.013617\n",
      "14     0.0  0.698718  0.935491\n",
      "15     0.0  0.701282  0.240274\n",
      "16     0.0  0.702564  0.119312\n",
      "17     0.0  0.703846  0.056903\n",
      "18     0.0  0.706410  0.322204\n",
      "19     0.0  0.834615  0.056216\n",
      "20     0.0  0.835897  0.083365\n",
      "21     0.0  0.837179  0.211147\n",
      "22     0.0  0.838462  0.136535\n",
      "23     0.0  0.839744  0.304082\n",
      "24     0.0  0.847436  0.301436\n",
      "25     0.0  0.848718  0.348248\n",
      "26     0.0  0.850000  0.099414\n",
      "27     0.0  0.851282  0.257718\n",
      "28     0.0  0.852564  0.190096\n",
      "29     0.0  0.855128  0.337333\n",
      "...    ...       ...       ...\n",
      "5850   1.0  0.687179  0.185614\n",
      "5851   1.0  0.688462  0.080061\n",
      "5852   1.0  0.689744  0.163563\n",
      "5853   1.0  0.697436  0.015219\n",
      "5854   1.0  0.698718  0.933026\n",
      "5855   1.0  0.701282  0.249357\n",
      "5856   1.0  0.702564  0.117621\n",
      "5857   1.0  0.703846  0.056068\n",
      "5858   1.0  0.706410  0.326404\n",
      "5859   1.0  0.834615  0.057098\n",
      "5860   1.0  0.835897  0.082791\n",
      "5861   1.0  0.837179  0.227628\n",
      "5862   1.0  0.838462  0.141279\n",
      "5863   1.0  0.839744  0.289003\n",
      "5864   1.0  0.847436  0.300218\n",
      "5865   1.0  0.848718  0.351119\n",
      "5866   1.0  0.850000  0.101053\n",
      "5867   1.0  0.851282  0.241194\n",
      "5868   1.0  0.852564  0.189513\n",
      "5869   1.0  0.855128  0.336966\n",
      "5870   1.0  0.857692  0.030845\n",
      "5871   1.0  0.858974  0.084603\n",
      "5872   1.0  0.860256  0.315883\n",
      "5873   1.0  0.885897  0.653803\n",
      "5874   1.0  0.903846  0.235958\n",
      "5875   1.0  0.910256  0.278299\n",
      "5876   1.0  0.989744  0.207921\n",
      "5877   1.0  0.991026  0.471966\n",
      "5878   1.0  0.998718  0.680809\n",
      "5879   1.0  1.000000  0.209008\n",
      "\n",
      "[5880 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    session.run(local_init)\n",
    "\n",
    "    num_batches = int(matrix.shape[0] / batch_size)\n",
    "    matrix = np.array_split(matrix, num_batches)\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        avg_cost = 0\n",
    "\n",
    "        for batch in matrix:\n",
    "            _, l = session.run([optimizer, loss], feed_dict={X: batch})\n",
    "            avg_cost += l\n",
    "\n",
    "        avg_cost /= num_batches\n",
    "\n",
    "        print(\"Epoch: {} Loss: {}\".format(i + 1, avg_cost))\n",
    "\n",
    "        # if i % display_step == 0 or i == 1:\n",
    "        #     print('Step %i: Minibatch Loss: %f' % (i, l))\n",
    "\n",
    "    print(\"Predictions...\")\n",
    "\n",
    "    matrix = np.concatenate(matrix, axis=0)\n",
    "\n",
    "    preds = session.run(decoder_op, feed_dict={X: matrix})\n",
    "\n",
    "    # print(matrix)\n",
    "    # print(preds)\n",
    "    \n",
    "    predictions = predictions.append(pd.DataFrame(preds))\n",
    "\n",
    "    predictions = predictions.stack().reset_index(name='rating')\n",
    "    predictions.columns = ['user', 'book', 'rating']\n",
    "    predictions['user'] = predictions['user'].map(lambda value: users[value])\n",
    "    predictions['book'] = predictions['book'].map(lambda value: books[value])\n",
    "\n",
    "    print(predictions)\n",
    "\n",
    "    keys = ['user', 'book']\n",
    "    i1 = predictions.set_index(keys).index\n",
    "    i2 = df.set_index(keys).index\n",
    "\n",
    "    recs = predictions[~i1.isin(i2)]\n",
    "    recs = recs.sort_values(['user', 'rating'], ascending=[True, False])\n",
    "    recs = recs.groupby('user').head(k)\n",
    "    recs.to_csv('prediction.csv', sep=',', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['user'] = df['user'] * user_range + user_min\n",
    "predictions['book'] = df['book'] * book_range + book_min\n",
    "\n",
    "pred = predictions.sort_values(['user', 'rating'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>book</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2380.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>0.301436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2380.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>0.277618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2380.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>0.216074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2380.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>0.099414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2380.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>0.030696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user   book    rating\n",
       "144  2380.0  670.0  0.301436\n",
       "195  2380.0  677.0  0.277618\n",
       "127  2380.0  382.0  0.216074\n",
       "146  2380.0  662.0  0.099414\n",
       "110  2380.0  375.0  0.030696"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.loc[pred['user'] == 2380].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144    670.0\n",
       "195    677.0\n",
       "127    382.0\n",
       "146    662.0\n",
       "110    375.0\n",
       "Name: book, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.loc[pred['user'] == 2380]['book'].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
